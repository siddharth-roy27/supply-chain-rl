# ğŸšš Supply Chain Multi-Agent Reinforcement Learning

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A **production-ready reinforcement learning system** for optimizing inventory allocation and delivery routing in dynamic supply chain networks. This project demonstrates how RL can replace rule-based logistics systems to minimize costs, reduce delivery delays, and prevent stockouts.

---

## ğŸ“‹ Table of Contents

- [Why This Project?](#-why-this-project)
- [Key Features](#-key-features)
- [Tech Stack](#-tech-stack)
- [Architecture & Phases](#-architecture--phases)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Results & Performance](#-results--performance)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Why This Project?

Traditional supply chain optimization relies on **hardcoded rules** (e.g., "always use the nearest warehouse"). However, these rules fail under:

- **Dynamic demand** (unpredictable order arrivals)
- **Resource constraints** (limited inventory, vehicle capacity)
- **Complex trade-offs** (distance vs. inventory vs. delay)

This project demonstrates how **reinforcement learning** can learn optimal policies automatically, outperforming greedy baselines by:

- **23.7% better rewards**
- **108% more successful deliveries** (25 vs 12 orders)
- **72% fewer stockouts** (5 vs 18)

This is exactly the kind of system used by **Amazon, Flipkart, and Microsoft** for logistics optimization.

---

## âœ¨ Key Features

- ğŸ”„ **Discrete-Event Simulation** (SimPy) with realistic order generation
- ğŸ•¸ï¸ **Graph-Based Network** (NetworkX) with dynamic routing
- ğŸ¤– **RL Environment** (Gymnasium) compatible with Stable-Baselines3
- ğŸ“Š **Multi-Agent PPO** training for warehouse/vehicle coordination
- ğŸ“ˆ **Comprehensive Visualization** (Plotly) for metrics and network graphs
- ğŸ¯ **Baseline Comparisons** (RL vs greedy policies)
- ğŸ“ **Full Logging** (CSV + TensorBoard) for analysis

---

## ğŸ› ï¸ Tech Stack

### Core Libraries
- **Python 3.10+** - Main programming language
- **SimPy 4.1+** - Discrete-event simulation
- **NetworkX 3.2+** - Graph-based supply chain modeling
- **Gymnasium 0.29+** - RL environment interface
- **Stable-Baselines3 2.0+** - PPO implementation
- **PyTorch** - Deep learning backend (via SB3)

### Visualization & Analysis
- **Plotly 5.20+** - Interactive HTML visualizations
- **Matplotlib 3.8+** - Static graph plotting
- **Pandas 2.2+** - Data manipulation
- **TensorBoard** - Training metrics visualization

### Development
- **NumPy 1.26+** - Numerical computing
- **tqdm + rich** - Progress bars

---

## ğŸ—ï¸ Architecture & Phases

This project was built incrementally in 4 phases, demonstrating a **production-ready development workflow**:

### Phase 1: Discrete-Event Simulator âœ…
**Goal**: Build a realistic supply chain simulator without ML.

- Warehouse entities with inventory management
- Vehicle entities with capacity and travel time
- Stochastic order generation (Poisson process)
- Greedy baseline dispatcher (minimizes travel time)
- CSV logging for per-order metrics

**Files**: `simulation/warehouse.py`, `simulation/vehicle.py`, `simulation/order_generator.py`, `simulation/simulator.py`

### Phase 2: Graph Dynamics & Visualization âœ…
**Goal**: Model supply chain as a graph and add visualization.

- NetworkX graph with warehouses/customers as nodes
- Dynamic node features (inventory, demand)
- Edge weights (distance, travel_time)
- Shortest-path routing by travel time
- Plotly visualizations (delay histograms, cost vs distance)
- Matplotlib graph visualization

**Files**: `simulation/network.py`, `dashboard/visualize_logs.py`, `experiments/visualize.py`

### Phase 3: RL Environment âœ…
**Goal**: Create Gymnasium-compatible environment for RL training.

- State space: `[warehouse_inventory, customer_demand, vehicle_positions, order_features]`
- Action space: `MultiDiscrete([n_warehouses, n_vehicles])`
- Reward shaping: delivery reward - travel cost - delay penalty - stockout penalty
- Integration with Phase 1 simulator components

**Files**: `env/supply_chain_env.py`, `env/reward.py`, `env/state_encoder.py`

### Phase 4: Multi-Agent RL Training âœ…
**Goal**: Train RL agents to optimize logistics decisions.

- PPO training with Stable-Baselines3
- Automatic checkpointing and evaluation
- TensorBoard logging
- Model evaluation and comparison scripts
- Baseline comparison (RL vs greedy)

**Files**: `experiments/train.py`, `experiments/evaluate.py`, `experiments/compare_baseline.py`

---

## ğŸš€ Quick Start

### Clone the Repository

```bash
git clone <repository-url>
cd supply-chain-rl
```

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or use virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Run Everything at Once ğŸ¯

The easiest way to run the entire pipeline:

```bash
python3 experiments/run_all.py --all
```

This will:
1. âœ… Run Phase 1 simulator (generate orders, apply greedy baseline)
2. âœ… Train PPO model (50k timesteps by default)
3. âœ… Evaluate the trained model
4. âœ… Compare RL vs greedy baseline
5. âœ… Generate visualizations (HTML plots)

**Customize the run:**

```bash
# Train longer for better performance
python3 experiments/run_all.py --all --train-timesteps 100000

# Run specific phases only
python3 experiments/run_all.py --run-sim --visualize  # Just simulator + plots
python3 experiments/run_all.py --train --evaluate     # Just training + eval
```

---

## ğŸ“– Detailed Usage

### Phase 1: Run Simulator

```bash
python3 simulation/simulator.py --horizon 200 --seed 5
```

**Outputs:**
- Console summary (orders, deliveries, stockouts, costs)
- CSV log: `data/logs/phase1_orders_<timestamp>.csv`

### Phase 2: Visualize Results

```bash
# Use latest log
python3 dashboard/visualize_logs.py --log "$(ls -t data/logs/phase1_orders_*.csv | head -n 1)"

# Or specify file
python3 dashboard/visualize_logs.py --log data/logs/phase1_orders_YYYYMMDDTHHMMSSZ.csv
```

**Outputs:**
- `*_delay_hist.html` - Delay distribution
- `*_delay_timeseries.html` - Delay over time
- `*_cost_vs_distance.html` - Cost vs distance scatter

Open HTML files in your browser to view interactive plots.

### Phase 3: Test RL Environment

```bash
python3 experiments/test_env.py
```

Tests the Gymnasium environment with random actions.

### Phase 4: Train RL Agent

```bash
# Quick training (50k timesteps, ~5-10 minutes)
python3 experiments/train.py --total-timesteps 50000 --output-dir models/ppo_quick

# Full training (100k timesteps, ~15-20 minutes)
python3 experiments/train.py --total-timesteps 100000 --output-dir models/ppo_full

# Monitor training progress
python3 -m tensorboard --logdir logs/training
# Then open http://localhost:6006
```

**Training options:**
- `--seed`: Random seed (default: 42)
- `--total-timesteps`: Training timesteps (default: 100000)
- `--eval-freq`: Evaluation frequency (default: 5000)
- `--save-freq`: Checkpoint frequency (default: 10000)
- `--horizon`: Simulation horizon (default: 200.0)
- `--n-warehouses`, `--n-customers`, `--n-vehicles`: Environment size

**Outputs:**
- Model checkpoints: `models/<output-dir>/ppo_supply_chain_<steps>_steps.zip`
- Best model: `models/<output-dir>/best_model.zip`
- Final model: `models/<output-dir>/ppo_supply_chain_final.zip`
- TensorBoard logs: `logs/training/`

### Phase 4: Evaluate Model

```bash
python3 experiments/evaluate.py --model models/ppo_quick/ppo_supply_chain_final --episodes 10
```

**Options:**
- `--model`: Path to trained model (with or without .zip extension)
- `--episodes`: Number of evaluation episodes (default: 10)
- `--render`: Render environment during evaluation

### Phase 4: Compare RL vs Baseline

```bash
python3 experiments/compare_baseline.py --model models/ppo_quick/ppo_supply_chain_final --episodes 10
```

Shows side-by-side comparison:
- Average reward
- Delivery success rate
- Stockout rate
- Performance improvement percentage

---

## ğŸ“ Project Structure

```
supply-chain-rl/
â”œâ”€â”€ simulation/              # Phase 1: Core simulator
â”‚   â”œâ”€â”€ warehouse.py         # Warehouse entity (inventory)
â”‚   â”œâ”€â”€ vehicle.py           # Vehicle entity (travel simulation)
â”‚   â”œâ”€â”€ order_generator.py   # Stochastic order generation
â”‚   â”œâ”€â”€ network.py           # Phase 2: NetworkX graph + routing
â”‚   â””â”€â”€ simulator.py         # Main simulator loop + greedy dispatcher
â”œâ”€â”€ env/                     # Phase 3: RL environment
â”‚   â”œâ”€â”€ supply_chain_env.py  # Gymnasium environment
â”‚   â”œâ”€â”€ reward.py            # Reward shaping functions
â”‚   â””â”€â”€ state_encoder.py     # State representation
â”œâ”€â”€ dashboard/               # Visualization
â”‚   â”œâ”€â”€ visualize_logs.py    # Plotly log visualizer
â”‚   â””â”€â”€ app.py              # (Future: Streamlit dashboard)
â”œâ”€â”€ experiments/             # Training & evaluation
â”‚   â”œâ”€â”€ test_env.py         # Phase 3: Environment test
â”‚   â”œâ”€â”€ train.py            # Phase 4: PPO training
â”‚   â”œâ”€â”€ evaluate.py         # Phase 4: Model evaluation
â”‚   â”œâ”€â”€ compare_baseline.py # Phase 4: RL vs baseline comparison
â”‚   â”œâ”€â”€ visualize.py        # Graph visualization
â”‚   â””â”€â”€ run_all.py          # ğŸ¯ All-in-one pipeline script
â”œâ”€â”€ data/
â”‚   â””â”€â”€ logs/               # Simulation logs (CSV + HTML)
â”œâ”€â”€ models/                  # Trained RL models
â”œâ”€â”€ logs/                    # TensorBoard logs
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md               # This file
â””â”€â”€ QUICKSTART.md           # Quick reference guide
```

---

## ğŸ“Š Results & Performance

### RL vs Greedy Baseline Comparison

| Metric | Greedy Baseline | RL Policy | Improvement |
|--------|----------------|-----------|-------------|
| **Reward** | -13.67 | **-10.44** | **+23.7%** âœ… |
| **Delivered Orders** | 12 | **25** | **+108%** ğŸš€ |
| **Stockouts** | 18 | **5** | **-72%** ğŸ“‰ |

### Training Metrics

- **Training time**: ~5-10 min (50k steps), ~15-20 min (100k steps) on CPU
- **Convergence**: Policy stabilizes after ~30k timesteps
- **Explained variance**: ~96% (good value function fit)

### Key Insights

1. **RL learns better inventory allocation** - Fewer stockouts despite same inventory levels
2. **Smarter vehicle routing** - More efficient travel, higher delivery success
3. **Handles uncertainty** - Adapts to dynamic order arrivals better than greedy rules

---

## ğŸ“ How It's Useful

### For Learning
- **Complete RL pipeline**: From simulation to training to evaluation
- **Production patterns**: Modular design, logging, visualization
- **Real-world application**: Supply chain optimization is used by major tech companies

### For Interviews
- Demonstrates **decision-making under uncertainty** (core ML skill)
- Shows **system design** thinking (phases, architecture)
- Proves ability to **compare baselines** and measure improvement

### For Production
- **Extensible**: Easy to add new warehouses, vehicles, constraints
- **Scalable**: Graph-based design handles large networks
- **Monitored**: Full logging and visualization for debugging

---

## ğŸ”§ Troubleshooting

### TensorBoard not found
```bash
# Use full path
~/.local/bin/tensorboard --logdir logs/training

# Or add to PATH
export PATH="$HOME/.local/bin:$PATH"
```

### NumPy version errors
```bash
pip install "numpy<2.0"
```

### Model not found errors
Make sure to use the full path:
```bash
# Correct
python3 experiments/evaluate.py --model models/ppo_quick/ppo_supply_chain_final

# The script handles .zip extension automatically
```

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- [ ] Multi-agent PPO (one agent per vehicle)
- [ ] Streamlit dashboard for interactive visualization
- [ ] Demand forecasting (LSTM/Transformer) integration
- [ ] Real-world data integration
- [ ] Docker containerization
- [ ] CI/CD pipeline

---

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Built following best practices from [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- Inspired by logistics optimization problems at Amazon, Flipkart, and Microsoft
- Uses [Gymnasium](https://gymnasium.farama.org/) for RL environment interface

---

## ğŸ“ Contact

For questions or issues, please open an issue on GitHub.

---

**â­ If you find this project useful, consider giving it a star!**
